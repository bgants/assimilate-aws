Saving transcription to 52weeksaws-dynamodb.mp4.json
Transcription downloaded to 52weeksaws-dynamodb.mp4.json
All right. I'm live with 52 weeks of aws still covering the aws developer certification, which is a great certification for people wanting to prove they know how to build stuff on aws in a software sense. And I'm gonna go ahead and get right into it into dynamodb and I'm gonna share my screen and talk about this in detail. So really to to dive into this Dynamodb is a pretty cool database. And there's a growth in unstructured data and you know, just people are talking about the fact that unstructured data is growing something like 40 to 60% every year. Things like rich media, audio, IOT data, like you name it, People are using unstructured data. So there are some limitations to using traditional relational databases which were built in, let's say, you know, initially papers around relational databases were from the 19 seventies. So what is dynamodb? Well, a few things to be aware of is it's a fully managed system. Uh And what that means is that it is a database system that uh is able to create a table. You can put a utilization for, it is fully managed service that will scale up and down. You don't have to worry about setting up distributed computing. It's it's all done for you. It also has a consistent and fast performance, so like millisecond level performance when servicing it. Uh and making a request or or putting things in there. There's access control as well. So you can set up things with the I am uh system here to let different people in your your organization use it in a different way. It's flexible, right? You can use things like the console, you can use the SDK. Right? So it's a it's a very user friendly database. I find it to be probably my favorite database to build solutions on AWS because it's just so quick now, in terms of the use case is a good one is dueling go. I've actually used it before for Portuguese. Uh Right, say thank you in Portuguese. I've also used spanish gracias have used the the app itself quite a bit and uh Dynamodb has 31 billion items, Right, Because they're building this thing for all these different languages and uh some of the use cases for dueling go where, you know, serverless web apps, micro service, data, mobile back ends, ad tech, gaming, right? These are all good use cases for for using dynamodb. A lot of startups that I worked at used dynamodb. So what are some of the key concepts of dynamo? Well, the first thing to be aware of is this concept of a table, Right, that's really like a spreadsheet in a sense, uh you know, the very simple level. Uh and there's items inside of the tables and these could be zero or there could be, you know, who knows, thousands of items inside or millions of items and then the attributes is each item uh that that basically each item is composed of one or more attributes. So these would be like the fundamental data element. Uh and then there's a primary key that would uniquely identify things. So no two items should have the same key. That's one key characteristics of something like dynamodb. There's a couple different kinds of keys as well. With dynamodb there's a single primary key. So this would be a partition key only. Uh in that scenario you would build an unaltered index on the primary key attribute, then there's also a composite primary key and this would be uh something where it's composed of a couple of attributes, like a partition key and sort key. And then dynamo would build up and then ordered index on the partition key attribute and assorted index. Uh And so really the big takeaway here is that dynamodb is a key value store in the document store and so you just choose what scenario you want to use it in now in terms of um dynamodb, it has some unique things as well. So like, unlike a relational database, dynamo doesn't have to have a predefined schema. That's really a huge advantage for many scenarios, you know, things like game data for example, you know, or metrics for a sporting event or something like that. And so if we look at an example of what that would look like there'd be adjacent data structure and inside there would be scalar types like number, string, you know, binary boolean noel multi value types, like string, set, number, set binary set or even list and map and each of the items can have a maximum of 400 kilobytes in size. So let's talk about partitions and data distribution, which is one of the key reasons to use uh something like dynamodb as it can scale automatically. So dynamo will store the data in a partition uh and that's really an allocation of storage for the table and it has SSD backing. So in the old days before there was salty drives, this was a huge problem. You know, the disk drive was in relational databases in particular was just really a problem. And so you have to have like really high memory machines now that we have SSD and we have key value based systems, we have really great performance and if your, your table in dynamo is a simple primary key, then dynamo will retrieve it based on a partition key value. And the partition key of the item is also known as a hash table. So you know in a sense it works like a python dictionary, write a python dictionary has the one um big o notation. What that means is that if there's five items where there's five million items in a python dictionary, you'll always get the same response because you just do a key look up, you just say does this key exist and it says yep it does exist. And so this is one of the advantages of using a key value store system on the flip side. Um if the table has a composite primary key, this would be a partition key and a sort key, then dynamo would store all the items with the same partition key value close together and then order them by the sort key value in the partition. And the sort key of an item is also known as the range attribute. So here's a good example to talk about is let's say you had a pets table and there's a simple key and it spans multiple partitions. The dynamo db system itself, the engine would calculate the hash value of the partition key to determine which partition should contain the item. So it's all happening but you know, really behind the scenes and then it's based on the hash value of the string, dog and the items are not stored in a sorted order. Right? And that's one of the advantages of it. But let's say you wanted to have a pets table that has a composite primary key that has let's say animal type, which would be a partition key and there's a name with a sort key. So dynamo again would calculate the hash value of a partition key to determine the partition that should contain the item and in the partition itself there'll be several items with the same partition key value. So dynamo would store a new item, among other items with the same partition key and then make that in ascending order by sort key. And in reality dynamo, that would then write an item with a partition key value of a dog and a short key value of Fido in ascending order. And so that's really the basics of dynamo. But let's talk a little bit now about secondary indexes. Dynamo allows you to have uh fast access to items by specifying the primary key. But there's also places where you would want to have one or more secondary, basically alternate keys available. And one way to do that is by having a secondary index on the table. So it's like a look up. You already have the metadata about what's happening in the system. So how would we do this? Well, one of the ways to do this would be to have a secondary key that's enabled uh and this would allow you to perform queries on attributes that are not part of the table's primary key. So a secondary index would let you query for example, data that's on the table by using alternate key. In addition to alternate key though, you could also have a subset of other table attributes. So you could specify what are the kind of attributes you want to copy and you know, basically customize it for your use case and there's a couple of different ways to do secondary indexes. There's a global secondary index which is an index with a partition key and a sort key, both which are different than the base table. And a global secondary index is considered global because the queries can span all the data in the base table across all the partitions. And a global secondary index has no size limitations and its provisions through settings for read and write activity. So really you can control, you know how that actually works in terms of, you know, the resources it uses, there's a local secondary index which is option two and that's an index that has the same partition key as the base table, but it's different sort key. A local secondary index is local in the sense that every partition of a local secondary index is scoped to the base table. And so what this means is that the total size for an index item for any one partition key can exceed 10GB. Uh and also a local secondary index uh share would need to be provisioned through settings for the read and write activity with the table. It's actually indexing. So in a nutshell, every table in dynamo has a limit of 20 global secondary indexes and then five local secondary indexes per table. So depending on what it is you're trying to do, you would use a combination of both. And in fact, if you had, For example, um 20, you know, global secondary indexes, you may then want to go into the local secondary indexes and also start to supplement what you're doing in a huge, you know scenario where you have lots of use cases for secondary indexes. Another example of a secondary index would be a music table where you query items by an artist, uh you can't do this query with the music table if you want to look at genre and album title or something like that. And so you could create a global secondary index on the genre and the album table from the music table, that would be a good example. Uh And one other thing to note is that if there is a combination of the genre and album table table title that aren't unique, that could be an issue. And so multiple albums can actually belong to one genre. So this is where you would query by the artist and the album title and you create a local secondary index called artist album title from the base music table and the local secondary index would have the same partition key as the base table. So let's talk a little bit about read and write throughput when, when building things with dynamodb and a few things to be aware of here, that dynamo will automatically replicate across the availability zones. Again, availability zones are essentially isolated. Uh parts of AWS, there's in a region, a geographic region, there's multiple availability zones. And so dynamo can automatically have support for this and because it's eventually consistent, which means that it can handle, you know, basically high load or failure, eventually everything will kind of sort itself out. Uh and as a as a result it has eventually consistent reads. So when you read data from dynamo, it initially may not reflect all of the write operations, but if you, if you quit a few times eventually it will and then it also has strongly consistent reads. So if you needed to, every time you query it like on only the first time you need to have a strongly consistent reads, you could tweak things so they could support that. So it really depends on which trade off you want. Do you want like basically the ability to have really high throughput or do you care about consistent reads? Because there's a trade off, if you have consistent read, you're gonna affect the performance a little bit. The amazon dynamodb transactions are really a pretty easy way to simplify developer experience as well. So you can do or nothing changes to multiple items, you can have transactions that have uh you know, a thomas, ademas, ethnicity, consistency, isolation, durability acid, right. Uh those are all available use cases would be like, you know, the typical use cases finance right? Like transactions you you want payroll or some kind of, you know, ledger thing to, to go through. Also thing to be aware of is that you can also do provisioned throughput or on demand. Throughput provisioned is actually basically ahead of time, you're you're going to say, look, I want to have this kind of capacity for my dynamo system. Uh And that's actually a pretty good use case for, you know, applications where there's a really consistent workload, but if you do on demand, dynamo can also basically automatically elastically respond back to whatever it is that you're building. Which is also a pretty interesting option. So on demand is a better option if you have new cables and you don't know yet, you know how many people are going to use it? You have, you know, non deterministic or unpredictable traffic or if you prefer to only pay for what you need, right? Like you you may not want to spend a lot of money, you know, there's definitely use cases where you want to do the opposite. If you're gonna do a million dollar super super bowl commercial you probably do want to provision ahead of time if you don't know yet if it's going to be successful or going to get a lot of traffic. The on demand option would be a good option. So a few things to remember is like, let's say you want to do, you know, read 20 items that are 11 kilobytes in size every second with eventual consistency. How many R. C. Used you need, you could say I need 30. So you would round up the 11 kilobytes up to the next multiple. So say 12 divided by four kilobytes per per R. C. U. Which would be three multiply the items by Read per second, which would be 60 divided by two for eventual consistency and that would be 30. Right? So there's a there's a kind of a rule of thumb that you can use to figure out the number of RCUS necessary. And if you wanted to do 120 items that are seven kilobytes in size every minute, how many W C us? Right, right. See us, do you need you need 14? It would be wcs require to write each seven kilobytes would be seven. Number of items per second would be to multiple W C u per item by items per second, which would be 14. Right, So it's nice that you can actually have a little bit of back of the envelope stuff to figure out how much you would need for either the read compute units or the right compute units. Now, let's dive into the streams and global tables. So a lot of applications can benefit from the ability to capture changes to items that are stored in dynamo at the point in time when these changes occur. And so one of the ways you can do this is by enabling dynamodb streams as a solution. A stream would consist of stream records. So a stream record represents a single data modification of the dynamodb table and the stream records are organized into groups or shards and each shard would act as a container for multiple stream records. Dynamo would automatically, then essentially split it out and spread that traffic across all the tables so that it would automatically handle everything that needed to be handled in terms of distributing that. And the global tables would be a fully managed solution for multi region, multi active database. Right? So you can build global scale solutions with dynamodb and a global table is a collection of one or more tables called a replica table. So this is kind of a cool concept, you know, lots of tables, they all kind of work together here. And the global table would use the dynamodb streams to essentially propagate the changes between the different replicas. Uh, but dynamo db does not support the partial replication though, of some items. So it's really kind of an all or nothing approach. Now, there is this concept of um, you know, in a global table, there would be conflicts and this is typical in a, in a in a distributed system. And so to achieve that, dynamo uses something called last writer wins. So basically dynamo would make a best effort to figure out who's the last person that made an update they look at, let's say a timestamp, make sure everybody agrees and that would be actually the way to, to choose what would be the the winner in a multi right situation with a conflict. A good example of, you know, how this would work with read consistency would be an application could read and write data to any replica, but if the application only uses eventually consistent reads and the only issues reads against one region, then then there's nothing to do. Right? It'll work without any modification. Now, strongly consistent reads though, require using a replica in the same region where the client is running uh and transactions are enabled for all single region dynamodb tables. So this is a kind of a common pattern actually for even a traditional sequel database, right? Like you would have multiple read replicas and people would potentially even be given a particular read replica because that are in one part of the country and that that actually scenario works out very well. Alright, let's talk about backup and restore. This is always a big one. Right? How do you actually, you know, deal with backup and restore. So one of the things you can do is have on demand backups. So amazon dynamodb provides this concept of on demand backup and restore, which is amazing, right? You can create a full backup so that you're gonna gonna rest at night. You also have the concept of point in time recovery, which is pretty neat as well. So you can actually recover from maybe like an accidental write or delete operation. So you don't necessarily with a point in time recovery, have to worry about the creation maintaining or scheduling of on demand backups. Let's talk about some of the basic operations for tables. So some of the things that come up quite a bit with dynamodb would be control operations. This is when you would go through and create or manage dynamo dynamodb tables. There's also data operations like you would, you know perform create, read update, delete operations, there's batch operations which would be like, you know, a stream to the table. For example transactions, you know, they're all or nothing kind of transaction with the A P I. The create table is a pretty cool one because it allows you to do an a sync operation. Right? So you're not locking the thing up, you're a synchronously writing a bunch of data. The boto three, you know the AWS SDK for python is a great way to do that. I personally use boto three with dynamo, I think it's a really great tool. The put item operation would create a new item or replace an old item with a new item. And so this would be a good thing to use if you want to actually just do updates the update item operation allows you to update an existing item without doing a bunch of back and forth things. And again, very easy to do with python. Right. Table dot update item and you just update the particular key value pair with a dictionary style update syntax. The delete item. Will I delete an item in a table by using primary keys. Again, primary key is is critical the put item update item. Delete item, You can also do a condition. Right? So you can match some kind of condition. Also the get item operation allows you to retrieve a specific item from a dynamodb table right? Which is often in the case of the fact that we know that each um you know idea is unique. You know it's a great way to actually get specific data from dynamodb. The query is going to be a little bit more expensive but allows you to read only the items that match the primary key from a table or a secondary index and you could do a filter expression. The scan is really a fairly expensive operation where you can actually scan the entire table. Now if it's a million row table that's probably not what you want to do. Um but you can get paginated results from doing the scan operation so to improve the performance and also the cost of what you're doing. Uh You can also basically have the number of items returned by query or scan have a pagination result or you can do a limit result like hey don't only show me this much stuff. You can also use batch operations to get like really high thorough put as well. Uh And then you can use dynamodb transactional, read, write a P. I. S. So developing with dynamo some of the things I would recommend doing would be you know creating a table. You can just do it right in the console, really easy as some data query it. Um and in general, I think the big things to take away with dynamo are that there are, you know, uh the introductory concepts, the key concepts like primary key, there's partitions, data distribution, there's secondary indexes, there's read, write, throughput, there's streams and global data, uh there's backup, restore and then there's a basic kind of python operations that you're doing. So. So really dynamo. If you're a developer, I think you must use dynamo in my opinion and I think a good way to to use it is to use the moto three A P I. And I would recommend doing things inside of cloud nine so that you get this really good e W s experience. So try to see see how you like dynamodb. It's really, I think a pretty cool tool and it's great for building fast prototypes. Alright. I will be covering next week cloudfront elastic cache, which is a pretty cool uh service that allows you to use cashing uh and we'll continue the AWS developer certification
